{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZhMCyB2IjePk7y+BoKIsU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanarayan/EPAM_PRACTICE/blob/main/PracticeEPAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuTwHbixw97v",
        "outputId": "16175658-a610-43c4-def1-bb0f5bc987b6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "The folder you are executing pip from can no longer be found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxGPRKL9qS_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d09c4de-37c6-4158-fa06-c34085d2f793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing SparkContext\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Check if a SparkContext already exists\n",
        "try:\n",
        "    sc = SparkContext.getOrCreate()\n",
        "    print(\"Using existing SparkContext\")\n",
        "except ValueError:\n",
        "    # If not, create a new one\n",
        "    sc = SparkContext(\"local\", \"FlatMap Example\")\n",
        "    print(\"Created a new SparkContext\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file as an RDD\n",
        "rdd = sc.textFile(\"/content/LinkedIn people profiles datasets.csv\")\n"
      ],
      "metadata": {
        "id": "dF7ElJSt1MO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dont consider the header\n",
        "header = rdd.first()\n",
        "rdd_no_header = rdd.filter(lambda line: line != header)\n",
        "print(f\"Total rows (Including header): {rdd.count()}\")\n",
        "print(f\"Total rows (excluding header): {rdd_no_header.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bE2T-tuz1NkO",
        "outputId": "1bf38e50-aa0a-4c0b-f582-53827038d9fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows (Including header): 1001\n",
            "Total rows (excluding header): 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split each row into columns\n",
        "rdd_split = rdd_no_header.map(lambda line: line.split(\",\"))\n",
        "\n",
        "header_columns = header.split(\",\")\n",
        "\n",
        "# Convert column names to lowercase and remove leading/trailing spaces for comparison\n",
        "header_columns = [col.strip('\"\"').strip().lower() for col in header_columns]\n",
        "\n",
        "print(\"Available Columns:\")\n",
        "for column in header_columns:\n",
        "    print(column)\n",
        "\n",
        "country_code_index = header_columns.index(\"country_code\")\n",
        "\n",
        "# Extract country codes from the relevant column\n",
        "country_codes_rdd = rdd_split.map(lambda row: row[country_code_index])\n",
        "\n",
        "# Get distinct country codes\n",
        "distinct_countries = (\n",
        "    rdd_split.map(lambda row: row[country_code_index].strip('\"').strip())  # Remove extra quotes\n",
        "    .distinct()\n",
        "    .collect()\n",
        ")\n",
        "\n",
        "#Print the distinct country codes\n",
        "print(\"Distinct Country Codes:\")\n",
        "for country in distinct_countries:\n",
        "    print(country)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PnIrrzErxUmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "region_index = header_columns.index(\"region\")\n",
        "regions_rdd  = rdd_split.map(lambda row: (row[region_index].strip('\"').strip(), 1))\n",
        "region_counts = regions_rdd.reduceByKey(lambda a, b: a + b)\n",
        "region_counts_result = region_counts.collect()\n",
        "for region, count in region_counts_result:\n",
        "    print(f\"Region: {region}, Count: {count}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "I53BvbCcSPQz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
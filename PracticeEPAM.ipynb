{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6c0yH6zpE/erZixCCoO1q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanarayan/EPAM_PRACTICE/blob/main/PracticeEPAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "kuTwHbixw97v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxGPRKL9qS_S"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "import csv,json , re\n",
        "from io import StringIO\n",
        "# Check if a SparkContext already exists\n",
        "try:\n",
        "    sc = SparkContext.getOrCreate()\n",
        "    print(\"Using existing SparkContext\")\n",
        "except ValueError:\n",
        "    # If not, create a new one\n",
        "    sc = SparkContext(\"local\", \"Linkdin Example\")\n",
        "    print(\"Created a new SparkContext\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd  = sc.textFile(\"/content/LinkedIn people profiles datasets.csv\")\n",
        "rdd_raw = rdd.map(lambda line: line.replace(',\"', '\\t\"'))\n",
        "print (rdd_raw.take(2))"
      ],
      "metadata": {
        "id": "dF7ElJSt1MO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header = rdd_raw.first()\n",
        "rdd_no_header = rdd_raw.filter(lambda line: line != header)\n",
        "print(f\"Total rows (Including header): {rdd_raw.count()}\")\n",
        "print(f\"Total rows (excluding header): {rdd_no_header.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bE2T-tuz1NkO",
        "outputId": "f1f1133f-8e1c-4078-d3b8-0cb3beccd40c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows (Including header): 1001\n",
            "Total rows (excluding header): 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "header_columns = header.split(\"\\t\")\n",
        "header_columns = [col.strip('\"\"').strip().lower() for col in header_columns]\n",
        "\n",
        "print(\"Available Columns:\")\n",
        "for idx, column in enumerate(header_columns):\n",
        "    print(f\"Index: {idx}, Column: {column}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "F65Tikfhw69Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rdd_split = rdd_no_header.map(lambda line: line.split(\",\"))\n",
        "\n",
        "def parse_csv_line(lines):\n",
        "    reader = csv.reader(lines, delimiter='\\t')  # Change delimiter if needed\n",
        "    return [row for row in reader]\n",
        "\n",
        "rdd_split = rdd_raw.filter(lambda row: row != header).mapPartitions(parse_csv_line)\n",
        "print (rdd_split.take(1))"
      ],
      "metadata": {
        "id": "xiaiIEDKzo7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_column(code):\n",
        "    cleaned_code = str(code).strip().strip('\"')\n",
        "    if not cleaned_code or cleaned_code.lower() in [\"null\", \"no data\",\"--\"]:\n",
        "        return \"Not Available\"\n",
        "    return cleaned_code"
      ],
      "metadata": {
        "id": "kZM3pqb4444I"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_digit(digit_value):\n",
        "    try:\n",
        "        return int(digit_value) if digit_value.isdigit() else 0\n",
        "    except ValueError:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "x07DzVzW9xEf"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "country_code_index = header_columns.index(\"country_code\")\n",
        "print(country_code_index)\n",
        "country_codes_rdd = rdd_split.map(lambda row: clean_column(row[country_code_index]))\n",
        "distinct_countries = country_codes_rdd.distinct().sortBy(lambda x: x.lower()).collect()\n",
        "\n",
        "print(\"Distinct Country Codes:\")\n",
        "for country in distinct_countries:\n",
        "    print(country)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PnIrrzErxUmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "region_index = header_columns.index(\"region\")\n",
        "regions_rdd  = rdd_split.map(lambda row: (clean_column(row[region_index]), 1))\n",
        "region_counts = regions_rdd.reduceByKey(lambda a, b: a + b)\n",
        "region_counts_result = region_counts.sortBy(lambda x: x).collect()\n",
        "\n",
        "print(\"Region:\" )\n",
        "for region, count in region_counts_result:\n",
        "    print(f\"{region}, Count: {count}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "I53BvbCcSPQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "company_name_index = header_columns.index(\"current_company:name\")\n",
        "company_names_rdd = rdd_split.map(lambda row: clean_column(row[company_name_index]))\n",
        "distinct_company_names_rdd = company_names_rdd.distinct().sortBy(lambda x: x.lower()).collect()\n",
        "\n",
        "print(\"Distinct company names:\")\n",
        "for company in distinct_company_names_rdd:\n",
        "    print(company)"
      ],
      "metadata": {
        "id": "_mqJhY5mwXz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "following_index = header_columns.index(\"following\")\n",
        "name_index = header_columns.index(\"name\")\n",
        "\n",
        "people_rdd = rdd_split.map(lambda row: (\n",
        "    clean_digit(clean_column(row[following_index])),\n",
        "    clean_column(row[name_index])\n",
        "))\n",
        "\n",
        "\n",
        "sorted_people_rdd = people_rdd.sortBy(lambda x: x[0], ascending=False)\n",
        "\n",
        "\n",
        "top_10_followed = sorted_people_rdd.take(10)\n",
        "\n",
        "\n",
        "print(\"Top 10 Most-Followed People:\")\n",
        "for following, name in top_10_followed:\n",
        "    print(f\"{name}: {following} followers\")\n"
      ],
      "metadata": {
        "id": "Gm2_BnZzytlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_string(raw_str):\n",
        "    \"\"\"Cleans raw JSON string by fixing formatting issues.\"\"\"\n",
        "    cleaned_str = raw_str.strip()  # Remove leading/trailing spaces\n",
        "    cleaned_str = cleaned_str.replace(\"\\t\", \",\")  # Fix delimiters\n",
        "    cleaned_str = cleaned_str.replace(\"['\", \"[\")  # Fix opening bracket\n",
        "    cleaned_str = cleaned_str.replace(\"']\", \"]\")  # Fix closing bracket\n",
        "    cleaned_str = cleaned_str.replace('\"\"', 'null')  # Handle empty values correctly\n",
        "    return cleaned_str\n",
        "\n",
        "def safe_json_loads(raw_str):\n",
        "    \"\"\"Safely loads JSON string, handling decoding errors.\"\"\"\n",
        "    try:\n",
        "        return json.loads(raw_str)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON Parse Error: {raw_str} | Error: {e}\")  # Debugging output\n",
        "        return None  # Return None for invalid JSON rows"
      ],
      "metadata": {
        "id": "WeO8koQuAM0D"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_index = header_columns.index(\"posts\")\n",
        "name_index = header_columns.index(\"name\")\n",
        "\n",
        "\n",
        "def extract_profile_likes(row):\n",
        "    \"\"\"Extracts profile name, falling back to 'name' column if attribution is missing.\"\"\"\n",
        "    post_list = safe_json_loads(clean_string(row[post_index]))\n",
        "\n",
        "    if post_list:\n",
        "        extracted_data = []\n",
        "        for post in post_list:\n",
        "            profile_name = post.get(\"attribution\", row[name_index]).replace(\"Liked by \", \"\") if post.get(\"attribution\") else row[name_index]\n",
        "            extracted_data.append((profile_name, 1))\n",
        "        return extracted_data\n",
        "    return []\n",
        "\n",
        "profile_likes_rdd = rdd_split.flatMap(extract_profile_likes)\n",
        "\n",
        "likes_count_rdd = profile_likes_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "all_profiles_rdd = rdd_split.map(lambda row: row[name_index]).distinct()\n",
        "\n",
        "likes_count_dict = dict(likes_count_rdd.collect())\n",
        "\n",
        "final_sorted_likes_count = sorted([(profile, likes_count_dict.get(profile, 0)) for profile in all_profiles_rdd.collect()], key=lambda x: x[0])\n",
        "\n",
        "print(\"Total Posts Liked Per Profile (Sorted by Name, Including 0 Likes):\")\n",
        "for profile, count in final_sorted_likes_count:\n",
        "    print(f\"{profile}: {count}\")"
      ],
      "metadata": {
        "id": "gTwxHkwn9xCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract column indices\n",
        "education_index = header_columns.index(\"education\")\n",
        "name_index = header_columns.index(\"name\")  # Profile name for association\n",
        "\n",
        "# Apply cleaning and JSON transformation\n",
        "education_rdd = rdd_split.map(lambda row: safe_json_loads(clean_string(row[education_index]))).filter(lambda x: x is not None)\n",
        "\n",
        "# Debugging output to verify JSON parsing\n",
        "print(\"Cleaned JSON Output:\", education_rdd.take(10))\n",
        "\n",
        "def extract_education_profiles(row):\n",
        "    \"\"\"Extracts education institutions from 'title' tag within JSON, linked to profiles.\"\"\"\n",
        "    education_list = safe_json_loads(clean_string(row[education_index]))  # Convert 'education' column to JSON\n",
        "\n",
        "    if education_list:  # Ensure JSON conversion is valid\n",
        "        extracted_data = []\n",
        "        for education in education_list:\n",
        "            institution = education.get(\"title\", \"Unknown Institution\")  # Get institution name\n",
        "            extracted_data.append((institution, 1))  # Count occurrences\n",
        "        return extracted_data\n",
        "    return []  # Return empty list if no valid education data\n",
        "\n",
        "# Create RDD mapping education institutions to profile counts\n",
        "education_count_rdd = rdd_split.flatMap(extract_education_profiles)\n",
        "\n",
        "# **Aggregate counts for each institution**\n",
        "institution_counts_rdd = education_count_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# **Extract all unique education institutions to ensure missing entries are shown with `0` counts**\n",
        "all_institutions_rdd = education_rdd.flatMap(lambda edu_list: [edu.get(\"title\", \"Unknown Institution\") for edu in edu_list]).distinct()\n",
        "\n",
        "# Convert counts to a dictionary for lookup\n",
        "institution_count_dict = dict(institution_counts_rdd.collect())\n",
        "\n",
        "# **Ensure missing institutions appear with a count of `0`**\n",
        "final_sorted_institutions = sorted([(institution, institution_count_dict.get(institution, 0)) for institution in all_institutions_rdd.collect()], key=lambda x: x[0].lower())\n",
        "\n",
        "# **Print final sorted results**\n",
        "print(\"Total Profiles by Education Institution (Sorted Alphabetically):\")\n",
        "for institution, count in final_sorted_institutions:\n",
        "    print(f\"{institution}: {count}\")"
      ],
      "metadata": {
        "id": "-w8ltzBG_EBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract column indices\n",
        "certification_index = header_columns.index(\"certifications\")  # Certification column\n",
        "\n",
        "# Apply cleaning and JSON transformation\n",
        "certification_rdd = rdd_split.map(lambda row: safe_json_loads(clean_string(row[certification_index]))).filter(lambda x: x is not None)\n",
        "\n",
        "# Debugging output to verify JSON parsing\n",
        "print(\"Cleaned JSON Output:\", certification_rdd.take(10))\n",
        "\n",
        "def extract_certification_profiles(row):\n",
        "    \"\"\"Extracts certification names from 'title' tag within JSON, linked to profiles.\"\"\"\n",
        "    certification_list = safe_json_loads(clean_string(row[certification_index]))  # Convert 'certification' column to JSON\n",
        "\n",
        "    if certification_list:  # Ensure JSON conversion is valid\n",
        "        extracted_data = []\n",
        "        for certification in certification_list:\n",
        "            cert_name = certification.get(\"title\", \"Unknown certifications\")  # Get certification name\n",
        "            extracted_data.append((cert_name, 1))  # Count occurrences\n",
        "        return extracted_data\n",
        "    return []  # Return empty list if no valid certification data\n",
        "\n",
        "# Create RDD mapping certifications to profile counts\n",
        "certification_count_rdd = rdd_split.flatMap(extract_certification_profiles)\n",
        "\n",
        "# **Aggregate counts for each certification**\n",
        "certification_counts_rdd = certification_count_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# **Extract all unique certifications to ensure missing entries are shown with `0` counts**\n",
        "all_certifications_rdd = certification_rdd.flatMap(lambda cert_list: [cert.get(\"title\", \"Unknown certifications\") for cert in cert_list]).distinct()\n",
        "\n",
        "# Convert counts to a dictionary for lookup\n",
        "certification_count_dict = dict(certification_counts_rdd.collect())\n",
        "\n",
        "# **Ensure missing certifications appear with a count of `0`**\n",
        "final_sorted_certifications = sorted([(cert_name, certification_count_dict.get(cert_name, 0)) for cert_name in all_certifications_rdd.collect()], key=lambda x: x[0].lower())\n",
        "\n",
        "# **Print final sorted results**\n",
        "print(\"Total Profiles by certifications (Sorted Alphabetically):\")\n",
        "for cert_name, count in final_sorted_certifications:\n",
        "    print(f\"{cert_name}: {count}\")"
      ],
      "metadata": {
        "id": "Bb_IOzYV-crh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommendations_index = header_columns.index(\"recommendations_count\")  # Recommendations count column\n",
        "\n",
        "# Function to parse and handle null values\n",
        "def extract_recommendations(row):\n",
        "    \"\"\"Extracts profile names and their corresponding recommendation counts.\"\"\"\n",
        "    name = row[name_index]  # Extract profile name\n",
        "    recommendations = row[recommendations_index]  # Extract recommendations count\n",
        "\n",
        "    # Convert recommendations to integer safely, treating null as 0\n",
        "    recommendations = int(recommendations) if recommendations and recommendations.isdigit() else 0\n",
        "\n",
        "    return (name, recommendations)\n",
        "\n",
        "# Create RDD with extracted recommendation counts\n",
        "recommendations_rdd = rdd_split.map(extract_recommendations)\n",
        "\n",
        "# **Aggregate total recommendations per profile**\n",
        "total_recommendations_rdd = recommendations_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# **Sort results alphabetically by profile name**\n",
        "sorted_recommendations = total_recommendations_rdd.sortByKey()\n",
        "\n",
        "# **Print final sorted results**\n",
        "print(\"Total Recommendations per Profile (Sorted Alphabetically):\")\n",
        "for name, count in sorted_recommendations.collect():\n",
        "    print(f\"{name}: {count}\")"
      ],
      "metadata": {
        "id": "7BNwEqC9_T5u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}